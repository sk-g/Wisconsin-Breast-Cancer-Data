{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum accuracy using Logistic Regression = 99.1228 with C = 6.250000\n",
      "Time taken for LogReg = 0.03 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD7CAYAAABjVUMJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgARkTz1/wFHSxYM48XKWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x238a4094a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fa542cc2c944>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bc.py\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(x)\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mtwo_layer_model\u001b[1;34m(data, features, n_h, learning_rate, num_iterations, print_cost)\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mlinear_activation_backward\u001b[1;34m(dA, cache, activation)\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mlinear_backward\u001b[1;34m(dZ, cache)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exec(open(\"bc.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings,math\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import operator,time,sys; import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn import linear_model,metrics\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV,train_test_split,KFold, cross_val_score\n",
    "import sklearn,matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report\n",
    "#from scipy.optimize import fmin_bfgs as bfgs\n",
    "from sklearn.metrics import log_loss\n",
    "#%matplotlib inline\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "def load(str):\n",
    "\n",
    "\tdf = pd.read_csv(str, header = 0)\n",
    "\tx = df.ix[:,df.columns!='diagnosis']\n",
    "\ty = df.ix[:,df.columns=='diagnosis']\n",
    "\t#print(y)\n",
    "\ty = y['diagnosis'].map({'M':1,'B':0})\n",
    "\t\n",
    "\tx = x.drop(['id','Unnamed: 32','area_mean','perimeter_mean','concavity_mean','concave points_mean','area_worst','perimeter_worst',\n",
    "\t\t'concave points_worst','concavity_worst','area_se','perimeter_se'],axis = 1)\n",
    "\tfeatures = []\n",
    "\tfor i in x:\n",
    "\t\tfeatures.append(i)\n",
    "\tframes = [x,y]\n",
    "\ttotal = pd.concat(frames,axis = 1)\n",
    "\t#print(total)\n",
    "\t#total.info()\n",
    "\treturn total, features\n",
    "\n",
    "def report(grid_scores , n_top = 3):\n",
    "    top_scores = sorted(grid_scores, key = operator.itemgetter(1), reverse= True)[:n_top]\n",
    "\n",
    "def gmm(x,features):\n",
    "\tdel features[-1]\n",
    "\ttrain, test = train_test_split(x, test_size = 0.2)\n",
    "\ttrain_x,test_x = train[features],test[features]\n",
    "\ttrain_y, test_y = train.diagnosis, test.diagnosis\n",
    "\tstart = time.time()\n",
    "\tx_all = np.r_[train_x,test_x]\n",
    "\tlowest_bic = np.infty\n",
    "\tbic = []\n",
    "\tn_components_range = range(1,7)\n",
    "\n",
    "\tcv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "\tfor i in cv_types:\n",
    "\t\tfor j in n_components_range:\n",
    "\t\t\tgmm = GMM(n_components= j, covariance_type = i,min_covar = 0.0001)\n",
    "\n",
    "\t\t\tgmm.fit(x_all)\n",
    "\t\t\tbic.append(gmm.aic(x_all))\n",
    "\t\t\tif bic[-1] < lowest_bic:\n",
    "\t\t\t\tlowest_bic = bic[-1]\n",
    "\t\t\t\tbest_gmm = gmm\n",
    "\tg = best_gmm\n",
    "\tg.fit(x_all)\n",
    "\n",
    "\tx=g.predict_proba(train_x)\n",
    "\n",
    "\tclf = RandomForestClassifier(n_estimators=1500, criterion='entropy', max_depth=15, min_samples_split = 2,min_samples_leaf=3, max_features='auto',\n",
    "\t n_jobs=-1,random_state=343)\n",
    "\tparam_grid = dict()\n",
    "\n",
    "\tgrid_search = GridSearchCV(clf, param_grid=param_grid, scoring='accuracy', cv = 5).fit(x,train_y)\n",
    "\t#print(grid_search.best_estimator_)\n",
    "\treport(grid_search.cv_results_)\n",
    "\tsvc = grid_search.best_estimator_.fit(x,train_y)\n",
    "\t###################\n",
    "\tgrid_search.best_estimator_.score(x, train_y)\n",
    "\tscores = cross_val_score(svc, x, train_y, cv=5, scoring='accuracy')\n",
    "\t#print(scores.mean(), scores.min())\n",
    "\t#print(scores)\n",
    "\tx_t = g.predict_proba(test_x)\n",
    "\n",
    "\ty_pred = grid_search.best_estimator_.predict(x_t)\n",
    "\tend = time.time()\n",
    "\tprint(\"Time taken for RandomForestClassifier with GMM = %.2f seconds\"%(end-start))\n",
    "\t#print(\"Accuracy using RandomForestClassifier with GMM = %.3f\"%(metrics.accuracy_score(y_pred,test_y)*100))\n",
    "\treturn(metrics.accuracy_score(y_pred,test_y)*100)\n",
    "\n",
    "def RandomForests(x,features):\n",
    "\t#RandomForests\n",
    "\tdel features[-1]\n",
    "\ttrain, test = train_test_split(x, test_size = 0.2)\n",
    "\ttrain_x,test_x = train[features],test[features]\n",
    "\ttrain_y, test_y = train.diagnosis, test.diagnosis\n",
    "\tstart = time.time()\n",
    "\tclf = RandomForestClassifier(n_estimators = 1000, max_features = 'auto', criterion = 'entropy',random_state = 30, min_samples_leaf = 5, n_jobs = 2)\n",
    "\tclf.fit(train_x,train_y)\n",
    "\tend = time.time()\n",
    "\ty_pred = clf.predict(test_x)\n",
    "\tacc_2 = metrics.accuracy_score(y_pred,test_y)\n",
    "\tprint(\"\\nTime taken for Random Forest Classifier = %.4f seconds\"%(end-start))\n",
    "\n",
    "\n",
    "\treturn acc_2*100\n",
    "\n",
    "def KNN():\n",
    "\t#K-Nearest Neighbors\n",
    "\tpass\n",
    "\n",
    "def LogReg(x, features):\n",
    "\t#Logistic Regression\n",
    "\tdel features[-1]\n",
    "\ttrain, test = train_test_split(x,test_size = 0.2) # 5 fold CV\n",
    "\ttrain_x, test_x = train[features], test[features]\n",
    "\ttrain_y, test_y = train.diagnosis,test.diagnosis\n",
    "\tstart = time.time()\n",
    "\tc_p = []\n",
    "\tacc_dict = {}\n",
    "\tt= 0.01\n",
    "\ttraining_accuracy = []\n",
    "\ttesting_accuracy = []\n",
    "\twhile t<=pow(10,4):\n",
    "\t\tc_p.append(t)\n",
    "\t\tt*=5\n",
    "\tfor c in c_p:\n",
    "\t\t#print(\"Running Log Reg with %.6f\"%(c))\n",
    "\t\tclf = LogisticRegression(C = c, penalty = 'l2')\n",
    "\t\t# training model using given data\n",
    "\t\tclf.fit(train_x,train_y)\n",
    "\n",
    "\t\t#training set accuracy\n",
    "\t\ty_pred_train = clf.predict(train_x)\n",
    "\t\tacc_train = metrics.accuracy_score(y_pred_train,train_y)\n",
    "\t\ttraining_accuracy.append(acc_train)\n",
    "\n",
    "\t\t# test set accuracy\n",
    "\t\ty_pred_test = clf.predict(test_x)\n",
    "\t\tacc_test = metrics.accuracy_score(y_pred_test,test_y)\n",
    "\t\ttesting_accuracy.append(acc_test)\n",
    "\t\tacc_dict[c] = acc_test\n",
    "\t\t#print(\"Accuracy = \",acc_test)\n",
    "\tend = time.time()\n",
    "\tprint(\"\\nMaximum accuracy using Logistic Regression = %.4f with C = %.6f\"%( max(acc_dict.items(),key = operator.itemgetter(1))[1]*100,\n",
    "\t\tmax(acc_dict.items(), key = operator.itemgetter(1))[0]))\n",
    "\tprint(\"Time taken for LogReg = %.2f seconds\"%(end-start))\n",
    "\t#print(c_p) \n",
    "\tplt.plot(c_p,training_accuracy, label = 'Training Accuracy')\n",
    "\tplt.plot(c_p,testing_accuracy, label = 'Testing Accuracy')\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "def ADB():\n",
    "\t#adaBoost\n",
    "\tpass\n",
    "\n",
    "def SVM(x,features,kernel = 'rbf'):\n",
    "\tdel features[-1]\n",
    "\t#for pos,val in enumerate(features):\tprint(pos,val)\n",
    "\ttrain, test = train_test_split(x, test_size = 0.2)\n",
    "\ttrain_x,test_x = train[features],test[features]\n",
    "\ttrain_y, test_y = train.diagnosis, test.diagnosis\n",
    "\t#for pos,val in enumerate(train_x):\tprint(pos,val)\n",
    "\t#print(train_x.shape)\n",
    "\t#print(test_x.shape)\n",
    "\tkernel = str(kernel)\n",
    "\tstart = time.time()\n",
    "\tc_param_range = []\n",
    "\tt=0.01\n",
    "\twhile t<=pow(10,1):\n",
    "\t\tc_param_range.append(t)\n",
    "\t\tt*=2\n",
    "\n",
    "\n",
    "\tresults_table_svm = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Accuracy'])\n",
    "\tresults_table_svm['C_parameter'] = c_param_range\n",
    "\tj = 0\n",
    "\t\n",
    "\tacc_dict_svm={}\n",
    "\tfor c_param in c_param_range:\n",
    "\t\t#print(\"Running SVM with  C = %f \"%(c_param))\n",
    "\t\t\t\n",
    "\t\tclf = BaggingClassifier(SVC(C = c_param, kernel = kernel),n_jobs=-1)\n",
    "\n",
    "\t\tclf.fit(train_x,train_y)\n",
    "\n",
    "\t\ty_pred = clf.predict(test_x)\n",
    "\n",
    "\t\tacc = metrics.accuracy_score(y_pred,test_y)\n",
    "\n",
    "\n",
    "\t\tacc_dict_svm[c_param]=acc\n",
    "\t\tresults_table_svm.ix[j,'Accuracy'] = acc\n",
    "\t\tj += 1\n",
    "\n",
    "\n",
    "\tbest_c_svm = results_table_svm.loc[results_table_svm['Accuracy'].idxmax()]['C_parameter']\n",
    "\t#print(\"Best Accuracy: %f with C= %.4f using %s kernel\"%(max(acc_dict_svm.items(),key = operator.itemgetter(1))[1]*100,\n",
    "\t#\tmax(acc_dict_svm.items(),key = operator.itemgetter(1))[0],kernel))\n",
    "\tend = time.time()\n",
    "\t#print(\"Average time taken for SVM = %.4f seconds\"%((end-start)/len(c_param_range)))\n",
    "\treturn acc_dict_svm[best_c_svm]\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    #assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "\n",
    "    np.random.seed(13456)\n",
    "    \n",
    "\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01 /np.sqrt(n_h)\n",
    "    b1 = np.random.randn(n_h,1)*0.01/np.sqrt(n_h)\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01 /np.sqrt(n_y)\n",
    "    b2 = np.random.randn(n_y,1)*0.01 /np.sqrt(n_y)\n",
    "   \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "  \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)],parameters['b' + str(l)],activation='relu')\n",
    "        \n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)],parameters['b' + str(L)],activation='sigmoid')\n",
    "    caches.append(cache)    \n",
    "    #assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    #assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = (np.sum(dZ,axis = 1, keepdims = True)) / m\n",
    "    #print(db.shape)\n",
    "    dA_prev = np.dot(cache[1].T,dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    #assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    #assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,\n",
    "                                                                                                  current_cache,\n",
    "                                                                                                  \"sigmoid\")\n",
    "\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA{}\".format(l + 2)],\n",
    "                                                                    current_cache,\n",
    "                                                                    \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W{}\".format(l + 1)] - learning_rate * grads[\"dW{}\".format(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b{}\".format(l + 1)] - learning_rate * grads[\"db{}\".format(l + 1)]\n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    print(\"Test Accuracy: \"  + str(np.sum((p == y)*100/m)))\n",
    "        \n",
    "    return np.sum((p == y)*100/m)\n",
    "\n",
    "def two_layer_model(data, features,n_h, learning_rate = 0.015, num_iterations = 30000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    del features[-1]\n",
    "    train, test = train_test_split(data, test_size = 0.2)\n",
    "    train_x,test_x = train[features],test[features]\n",
    "    train_y, test_y = train.diagnosis.values.reshape((train.diagnosis.shape[0],1)),test.diagnosis.values.reshape((test.diagnosis.shape[0],1))\n",
    "    train_x,test_x = train_x.T,test_x.T\n",
    "    train_y = train_y.T\n",
    "    test_y = test_y.T\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = train_x.shape[1]                           # number of examples\n",
    "    n_x = train_x.shape[0]\n",
    "    n_h = n_h\n",
    "    n_y = train_y.shape[0]\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(train_x, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2, train_y)\n",
    "        ### END CODE HERE ###\n",
    "        if cost <= 0.1:\n",
    "        \tbreak\n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(train_y, A2) - np.divide(1 - train_y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "\n",
    "    \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(train_x, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    #print(p.shape)\n",
    "    #print('\\n',test_y.shape)\n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Training Accuracy: \"  + str(np.sum((p == train_y)*100)/455))\n",
    "    a = predict(test_x,test_y,parameters)\n",
    "    end = time.time()\n",
    "    print(\"Time taken: %.4f\"%(end-start))\n",
    "    return a\n",
    "    ###### plot the cost   #######\n",
    "\"\"\"\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\t\n",
    "\n",
    "def L_layer_model(data, features, layers_dims, learning_rate = 0.015, num_iterations = 5000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    del features[-1]\n",
    "    train, test = train_test_split(data, test_size = 0.2)\n",
    "    train_x,test_x = train[features],test[features]\n",
    "    train_y, test_y = train.diagnosis.values.reshape((train.diagnosis.shape[0],1)),test.diagnosis.values.reshape((test.diagnosis.shape[0],1))\n",
    "    train_x,test_x = train_x.T,test_x.T\n",
    "    train_y = train_y.T\n",
    "    test_y = test_y.T\n",
    "    np.random.seed(11)\n",
    "    costs = []                         # keep track of cost\n",
    "    m = train_x.shape[1]\n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(train_x, parameters)\n",
    "\n",
    "        # Compute cost.\n",
    "\n",
    "        cost = compute_cost(AL, train_y)\n",
    "        if cost < 0.1:\n",
    "        \tpass\n",
    "    \n",
    "        # Backward propagation.\n",
    "\n",
    "        grads = L_model_backward(AL, train_y, caches)\n",
    "\n",
    " \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "              \n",
    "\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "        p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "        probas, caches = L_model_forward(train_x, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "        \tp[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Training Accuracy: \"  + str(np.sum((p == train_y) * 100 )/455))\n",
    "    predict(test_x,test_y,parameters)\n",
    "    end = time.time()\n",
    "    #print(\"Time taken: %.4f\"%(end-start))\n",
    "    ###### plot the cost   #######\n",
    "\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "def NeuralNetwork(x,features):\n",
    "\t# Neural Network with one hidden layer\n",
    "\t# Will try and work with more hidden layers\n",
    "\t# Optimization of hyperparameters for one hidden layer NN supported\n",
    "\t# WORK IN PROGRESS\n",
    "\tdel features[-1]\n",
    "\ttrain, test = train_test_split(x, test_size = 0.2)\n",
    "\ttrain_x,test_x = train[features],test[features]\n",
    "\ttrain_y, test_y = train.diagnosis.values.reshape((train.diagnosis.shape[0],1)),test.diagnosis.values.reshape((test.diagnosis.shape[0],1))\n",
    "\ttrain_y = train_y.T\n",
    "\ttest_y = test_y.T\n",
    "\t\"\"\"\n",
    "\t# Checking dimensions\n",
    "\tprint(test_y, '\\n',test.diagnosis)\n",
    "\tprint(\"shape of train_x = \",train_x.shape)\n",
    "\tprint(\"shape of test_x = \",test_x.shape)\n",
    "\tprint(\"shape of train_y = \",train_y.shape)\n",
    "\tprint(\"shape of test_y = \",test_y.shape)\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# DEFINING PARAMETERS \n",
    "\tn_h = 5 # number of hidden units\n",
    "\t\n",
    "\t# layer 1 (input layer)\n",
    "\n",
    "\tw1 = np.random.randn(n_h,train_x.shape[1])*0.01/np.sqrt(train_x.shape[0])\n",
    "\tb1 = np.random.randn(n_h,1)*0.01\n",
    "\tw2 = np.random.randn(train_y.shape[0],n_h)*0.01/np.sqrt(train_y.shape[0])\n",
    "\tb2 = np.random.randn(train_y.shape[0],1)*0.01\n",
    "\n",
    "\tnum_iters = 10\n",
    "\tlearning_rate = 0.1\n",
    "\tm = (1/train_y.shape[1])\n",
    "\tfor i in range(num_iters):\n",
    "\t\t#print(w1.shape,w2.shape,b1.shape,b2.shape) # checkind dimensions\n",
    "\n",
    "\t\t###### FORWARD PROPAGATION #######\n",
    "\n",
    "\t\tz1 = np.dot(w1,train_x.T) + b1 # linear transform\n",
    "\t\tA1 = np.maximum(z1,0.001*z1) # ReLu Activation or maybe we can use leaky ReLu \n",
    "\t\tz2 = np.dot(w2,A1) + b2\n",
    "\t\tA2 = (1/(1+np.exp(-z2))) # sigmoid activation to get y hat \n",
    "\n",
    "\t\t\n",
    "\t\t#print(np.log(abs(1-A2)))\n",
    "\t\t#cost = -(1/m)*np.sum(np.dot(train_y,np.log(A2).T)+np.dot(1-train_y,np.log(1-A2).T))\n",
    "\t\tcost = log_loss(train_y,A2) # this is throwing some error, \n",
    "\t\t\t\t\t\t\t\t\t # no patiance to workaround, will write own loss function\n",
    "\t\t#print(z2)\n",
    "\t\t#print(\"Iteration %i, cost %.4f\"%(i,cost))\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t###### \tBACKWARD PROPAGATION #######\n",
    "\n",
    "\t\t#dA2 = train_y/A2 + (1-train_y)/(1-A2)\n",
    "\n",
    "\t\tdA2 = - (np.divide(train_y, A2) - np.divide(1 - train_y, 1 - A2))\n",
    "\t\t#print(dA2)\n",
    "\t\tdz2 = (dA2*(z2*(1-z2))) # deriative of sigmoid \n",
    "\n",
    "\t\tdw2 = (np.dot(dz2,A1.T))/m\n",
    "\n",
    "\t\t# print(w2.shape,dw2.shape) # checking fwd bwd param dims\n",
    "\n",
    "\t\tdb2 = np.sum(dz2, axis = 1, keepdims = True) / m\n",
    "\n",
    "\t\tdA1 = np.dot(w2.T,dz2)\n",
    "\n",
    "\t\t\n",
    "\t\tdz1 = (dA1 ) #relu derivative (modified, ReLu is not differentiable at 0 )\n",
    "\n",
    "\t\tdw1 = np.dot(dz1,train_x) / m\n",
    "\n",
    "\t\tdb1 = np.sum(dz1, axis = 1, keepdims = True) / m\n",
    "\t\t#print(w1.shape,dw1.shape, b1.shape, db1.shape) # checking fwd bwd param dims\n",
    "\t\t\n",
    "\t\t##### UPDATING PARAMETERS ######\n",
    "\n",
    "\t\tw1 = w1 - learning_rate * dw1\n",
    "\t\tb1 = b1 - learning_rate * db1\n",
    "\t\tw2 = w2 - learning_rate * dw2\n",
    "\t\tb2 = b2 - learning_rate * db2\n",
    "\t\tcost2 = -(1/m)*np.sum(np.dot(train_y,np.log(A2).T))\n",
    "\t\t#print(\"Iteration %i, cost2 %.4f\"%(i,cost2))\n",
    "\n",
    "def call_function(x):\n",
    "\tdata, features =load('bc.csv')\n",
    "\t\"\"\" FUNCTION CALLS \"\"\"\n",
    "\tfunction = str(x)\n",
    "\t\n",
    "\tif function == \"LR\":\n",
    "\t\tLogReg(data,features)\n",
    "\telif function == \"NN\":\n",
    "\t\t#hidden_unit_size = [5 , 10 , 15 , 20 , 25 , 50 , 100 , 200 , 500, 1000]\n",
    "\t\t#hidden_unit_size = [5 , 10 , 15 , 20 , 25 , 26, 27, 28, 29, 30]\n",
    "\t\tarr_res=[]\n",
    "\t\t#parameters = two_layer_model(data, features,100, learning_rate = 0.0050, num_iterations = 30000, print_cost = True)\n",
    "\t\tstart = time.time()\n",
    "\t\tfor i in range(1,50):\n",
    "\t\t\tdata, features =load('bc.csv')\n",
    "\t\t\tprint('\\n',i)\n",
    "\t\t\tparameters = two_layer_model(data, features,i, learning_rate = 0.0035, num_iterations = 30000, print_cost = False)\n",
    "\t\t\tarr_res.append(parameters)\n",
    "\t\tend = time.time()\n",
    "\n",
    "\t\tprint(\"Time taken = %.4f\"%(end-start))\n",
    "\t\tplt.plot([i+1 for i in range(1,50)],arr_res,'ro')\n",
    "\t\tplt.ylabel(\"Test Accuracy\")\n",
    "\t\tplt.xlabel(\"Hidden Unit Size\")\n",
    "\t\tplt.show()\n",
    "\t\t\n",
    "\telif function == \"GMM\":\t\n",
    "\t\tprint(\"\\nAccuracy with Random Forest Classifier using GMM = %.4f\"%(gmm(data,features)))\n",
    "\telif function == \"RFC\":\t\n",
    "\t\tprint(\"\\nAccuracy with Random Forest Classifier = \",RandomForests(data , features))\n",
    "\telif function == \"SVM\":\t\n",
    "\t\tkernels = ['rbf','linear','poly','sigmoid']\n",
    "\t\tLogReg(data,features)\n",
    "\t\tfor i in kernels:\t\n",
    "\t\t\tstart = time.time()\n",
    "\t\t\tprint(\"\\nAccuracy using SVM with \",i,\":\",SVM(data,features,i)*100)\n",
    "\t\t\tend = time.time()\n",
    "\t\t\tprint(\"Time taken for \",i,\" = %.4f seconds\"%(end-start))\n",
    "\telif function == \"DNN\":\n",
    "\t\tlayers_dims = [19, 44, 44, 44, 44, 44, 48, 48, 48, 48, 44, 44, 44, 44, 44, 1]\n",
    "\t\tL_layer_model(data, features, layers_dims, learning_rate = 0.01, num_iterations = 15000, print_cost = True)\t\t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # LR = Logistic Regression\n",
    "    # SVM = SVM\n",
    "    # NN = one hidden layer NN\n",
    "    # DNN = explicit number of hidden layers with L-1 ReLu and final Sigmoid layer\n",
    "    # RFC = Random Forest\n",
    "    # GMM = Random Forest with Gaussian Mixture\n",
    "\tcall_function(\"LR\")\n",
    "\tcall_function(\"NN\")\n",
    "\tcall_function(\"DNN\")\n",
    "\tcall_function(\"SVM\")\n",
    "\tcall_function(\"RFC\")\n",
    "\tcall_function(\"GMM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
